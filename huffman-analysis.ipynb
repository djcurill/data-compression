{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "048ae216",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Code Design Test: Data Compression Design\n",
    "Design an algorithm that will compress a given data buffer of bytes. Please describe your design and submit an\n",
    "implementation in Python.\n",
    "Your submission will be judged based on\n",
    "- The number of bytes your output uses if saved to file\n",
    "- Run time\n",
    "- Scalability\n",
    "- Maintainability\n",
    "- Testability\n",
    "\n",
    "**Assumptions**\n",
    "1. data is an array of bytes. Each byte will contain a number from 0 to 127 (0x00 to 0x7F). It is common\n",
    "for the data in the buffer to have the same value repeated in the series.\n",
    "2. The compressed data will need to be decompressable. Please ensure that your algorithm allows for a\n",
    "decompression algorithm to return the buffer to its previous form.\n",
    "\n",
    "**Example**\n",
    "```python\n",
    "data = bytes([0x03, 0x74, 0x04, 0x04, 0x04, 0x35, 0x35, 0x64,\n",
    "0x64, 0x64, 0x64, 0x00, 0x00, 0x00, 0x00, 0x00,\n",
    "0x56, 0x45, 0x56, 0x56, 0x56, 0x09, 0x09, 0x09])\n",
    "compressed_bytes = byte_compress(data)\n",
    "```\n",
    "\n",
    "## Algorithm Design\n",
    "\n",
    "There are two important properties that can be leveraged to reduce the size of the data:\n",
    "1. The sequence of bytes consist of numbers that range from 0 - 127.\n",
    "2. Numbers tend to repeat themselves which suggests the distribution of integers will likely be skewed (i.e. not uniform). \n",
    "\n",
    "Two popular algorithms can be used to compress the data. Each will have their tradeoffs.\n",
    "1. Fixed length encoding \n",
    "2. Huffmaan enconding\n",
    "\n",
    "### Choosing an Algorithm\n",
    "#### Fixed Length Enconding\n",
    "**Pros**\n",
    "* Simple to implement\n",
    "* Performs better with greater entropy (i.e. more random distribution)\n",
    "\n",
    "**Cons**\n",
    "* Performs much worse given a skewed distribution\n",
    "\n",
    "\n",
    "#### Huffman Encoding\n",
    "**Pros**\n",
    "* Performs better given repeating values\n",
    "* Will perform similar to fixed encoding given large input sample size (will derive fixed encoding as n increases)\n",
    "\n",
    "**Cons**\n",
    "* Much more complex to implement\n",
    "* Performance will level out as sample size becomes large enough to include the full population (0-127)\n",
    "\n",
    "The huffman enconding technique offers more flexibility and will likely outperform the Fixed length encoding method should the data be much more likely to be skewed. This algorithm will not perform well if the distribution of data turns out to be uniform (i.e. a high degree of entropy). \n",
    "\n",
    "## How Huffman Enconding Works\n",
    "\n",
    "Huffman enconding takes advantage of repeating values. Given a sequence of bytes, any bytes that occur more frequently than other, will be assigned an encoding that uses a smaller number of bits. More rare occurences will use more bits, meaning they take up more space but occur less frequently. \n",
    "\n",
    "This algorithm can be broken up into steps:\n",
    "\n",
    "**Step 1**: Compute the probability distribution of the data.\n",
    "\n",
    "Source Code: [src.core.dist.py](https://github.com/djcurill/data-compression/blob/main/src/core/dist.py)\n",
    "\n",
    "Example:\n",
    "```python\n",
    ">>> from src.core.dist import Dist\n",
    ">>> Dist([1,1,1,2,3])\n",
    "{1: 0.6, 2: 0.2, 3: 0.2}\n",
    "```\n",
    "\n",
    "**Step 2**: Represent each item in the distribution as a node\n",
    "\n",
    "Source code: [src/core/node.py](https://github.com/djcurill/data-compression/blob/main/src/core/node.py)\n",
    "\n",
    "Nodes will be used to eventually create a tree structure. The following pseudcode describes the structure of a Node:\n",
    "\n",
    "```\n",
    "class Node:\n",
    "    count:float            # How often a symbol occurs (i.e. the probability)\n",
    "    symbol:Union[str,int]  # String representation of the symbol (in our case, this would be the byte)\n",
    "    left:Node              # Left child node\n",
    "    right:Node             # Right child node\n",
    "```\n",
    "\n",
    "Example:\n",
    "```python\n",
    ">>> from src.core.dist import Dist\n",
    ">>> Dist([1,1,1,2,3])\n",
    "{1: 0.6, 2: 0.2, 3: 0.2}\n",
    ">>> from src.core.node import Node\n",
    ">>> Node(count=0.6, symbol=0x01)\n",
    "Node(count=0.6, symbol=1)\n",
    "```\n",
    "---\n",
    "**Step 3**: Create a Huffman Tree\n",
    "\n",
    "Source code: [src/core/tree.py](https://github.com/djcurill/data-compression/blob/main/src/core/tree.py)\n",
    "\n",
    "Create a huffman given a distribution. This can be broken into steps\n",
    "1. Create a list of nodes from a distrbution\n",
    "2. Iterate over the list of nodes until eventually only the root remains\n",
    "3. In each iteration, take the two nodes with smallest `count` values and merge them.\n",
    "4. Append the merged node back into the list.\n",
    "5. Repeat\n",
    "6. When only one node remains, return the root node. This is the Huffman Tree.\n",
    "\n",
    "---\n",
    "**Step 4**: Create an encoding table\n",
    "\n",
    "Source code: [src/core/tree.py](https://github.com/djcurill/data-compression/blob/main/src/core/tree.py)\n",
    "\n",
    "An encoding table will map a `symbol` to its bit representation. In python, the [bitarray library](https://pypi.org/project/bitarray/) provides helper methods to represent objects as bits. The bitarray representation of a symbol is determined by the number of `lefts` and `rights` taken to navigate from the `root` node to a leaf node. This algorithm ensures that each symbol as a unique prefix when decoding (more on this later). \n",
    "\n",
    "To create an encoding table from a Huffman tree, you take the all the leaf nodes and add their `symbol -> bits` mapping to a dictionary. This can be done using a simple depth first search algorithm. Source code has been copy and pasted here. \n",
    "\n",
    "```python\n",
    "def get_encoding_table(self) -> Dict[str, bitarray]:\n",
    "    encoding_table = {}\n",
    "\n",
    "    def _dfs(tree:Node, code:str) -> None:\n",
    "        if tree.is_leaf_node():\n",
    "            # Must handle edge case where tree is only root node\n",
    "            code = code if code else \"0\"\n",
    "            encoding_table[tree.symbol] = bitarray(code)\n",
    "            return\n",
    "        if tree.left is not None:\n",
    "            _dfs(tree.left, code + \"0\")\n",
    "        if tree.right is not None:\n",
    "            _dfs(tree.right, code + \"1\")\n",
    "\n",
    "    if self.root is not None:\n",
    "        _dfs(self.root, code=\"\")\n",
    "    return encoding_table\n",
    "```\n",
    "\n",
    "---\n",
    "**Step 5**: Encoding Algorithm\n",
    "\n",
    "Source code: [src/lossless/encoders/huffman_encoder.py](https://github.com/djcurill/data-compression/blob/main/src/lossless/encoders/hoffman_encoder.py)\n",
    "\n",
    "This algorithm is really simple. Just take a symbol and map it to its `bitarray` representation. \n",
    "\n",
    "Example:\n",
    "```python\n",
    ">>> from src.lossless.encoders.hoffman_encoder import HuffmanEncoder\n",
    ">>> from src.core.tree import HoffmanTree\n",
    ">>> from src.core.dist import Dist\n",
    ">>> dist = Dist([1,1,1,1,2,2,2,3])\n",
    ">>> tree = HoffmanTree(dist)\n",
    ">>> encoder = HuffmanEncoder(tree)\n",
    ">>> encoder(1)\n",
    "bitarray('1')\n",
    "```\n",
    "\n",
    "---\n",
    "**Step 6:** Decoding Algorithm\n",
    "\n",
    "Source code: [src/lossless/decoders/huffman_decoder.py](https://github.com/djcurill/data-compression/blob/main/src/lossless/decoders/huffman_decoder.py)\n",
    "\n",
    "Decoding a sequence of bits is quite elegant with a Huffman Tree. Given a sequence of bits, you iteratively traverse a vit sequence and navigate the Huffman Tree until a leaf node has been reached. If the bit is `0` go the left child node. If the bit is `1`, go to the right child node. Each `symbol -> bitarray` mapping is guaranteed to be unique since the tree was constructed using the optimal merge pattern. That means no two symbols can have the same prefix of bits. Below is the implementation of the decoding algorithm:\n",
    "\n",
    "```python\n",
    "class HuffmanDecoder(Decoder):\n",
    "\n",
    "    def __init__(self, tree:HoffmanTree):\n",
    "        assert tree.root is not None, \"Must have non-null root to perform Huffman decoding\"\n",
    "        self.root = tree.root\n",
    "\n",
    "    def __call__(self, bits:bitarray, start:int=0) -> Tuple[Symbol, int]:\n",
    "        if start >= len(bits):\n",
    "            return None\n",
    "        pos = start\n",
    "        node = self.root\n",
    "        while not node.is_leaf_node():\n",
    "            bit = bits[pos]\n",
    "            node = node.left if bit == 0 else node.right\n",
    "            pos += 1\n",
    "        return node.symbol, pos\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Step 7:** Putting it all together\n",
    "\n",
    "See [main.py](https://github.com/djcurill/data-compression/blob/main/src/main.py) for the full implementation.\n",
    "\n",
    "## Running the algorithm\n",
    "\n",
    "To run the code using the example provided in the problem statement:\n",
    "```bash\n",
    "make run-example\n",
    "```\n",
    "\n",
    "To run a custom experiment:\n",
    "```bash\n",
    "make run-experiment SIZE=100 PROB=0.9\n",
    "```\n",
    "\n",
    "`SIZE` will control the number of bytes to be compressed and `PROB` is the probability of a byte repeating itself. The higher `PROB` is, the more skewed the distribution. \n",
    "\n",
    "The before the compression are the number of bytes to represent integers from 0-127 in python. \n",
    "\n",
    "The size after compressions is a bit more involved:\n",
    "\n",
    "\n",
    "\n",
    "## Learning & Limitations\n",
    "\n",
    "* Huffman encoding works very well with compression ratios in the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2229102",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
